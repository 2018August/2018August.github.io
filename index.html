<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="August,undefined"><meta name="copyright" content="August"><title>Technologies come and technologies go, but insight is forever. | 人工智能随笔</title><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.3"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  localSearch: undefined
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="http://p7l96nurm.bkt.clouddn.com/blog/180423/kC2iKEe0K1.jpg"></div><div class="author-info__name text-center">August</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">8</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">7</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">1</span></a></div></div></div><nav class="http://p7l96nurm.bkt.clouddn.com/blog/180426/jaAmCbe3cc.jpg" id="nav" style="background-image: url(http://p7l96nurm.bkt.clouddn.com/blog/180426/jaAmCbe3cc.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">人工智能随笔</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">人工智能随笔</div><div id="site-sub-title">Technologies come and technologies go, but insight is forever.</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item"><a class="article-title" href="/8. 玩转逻辑回归之金融评分卡模型/">玩转逻辑回归之金融评分卡模型</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-05-06</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/machine-learning/">machine learning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/机器学习/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/逻辑回归/">逻辑回归</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/评分卡模型/">评分卡模型</a></span><div class="content"><p>虽然现在出现了很多性能优秀的分类算法，包括svm，RF，GBDT，DNN等，作为最简单的分类算法，$lr$ 依然是工业界主流的分类算法之一。那么 $lr$ 到底有什么魔力，即使面对如此众多的 “高手” 面前，依然屹立不倒呢？</p>
<p>市面上关于 $lr$ 的书籍和文章大部分的讲解都是针对 $lr$ 一些基本理论或者一些推导公式。掌握这些还远远不够，要想让 $lr$ 发挥其最大效果，必须要有一套科学的、严密的数据预处理流程。</p>
<p>和市面上对 $lr$ 算法的讲解不同，本文将以金融评分卡模型为例，讲解一整套 $lr$ 配套的数据处理流程，包括数据获取，EDA (探索性数据分析)，数据预处理，到变量筛选，$lr$ 模型的开发和评估，生成评分卡模型。希望大家在阅读本篇文章之后能够轻松驾驭 $lr$ 算法。</p></div><a class="more" href="/8. 玩转逻辑回归之金融评分卡模型/#more">阅读更多</a><hr></div><div class="recent-post-item"><a class="article-title" href="/7. 攀登传统机器学习的珠峰-SVM (下)/">攀登传统机器学习的珠峰-SVM (下)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-05-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/machine-learning/">machine learning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/机器学习/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/svm/">svm</a></span><div class="content"><p>本节是SVM系列三部曲的最后一部分。主要讲了SMO算法，SVR算法，sklearn中SVM算法的参数介绍和调参建议。网上有很多SMO算法的文章，有的要么讲解的比较浅显，要么就是一堆公式的堆砌，让人看完之后会有各种疑问，比如：SMO算法和EM算法有什么异同点？坐标上升(下降)法与梯度下降(上升)法有什么异同点？什么时候用坐标上升(降法)？什么时候用梯度下降法(上升)？SMO为什么采用两个变量迭代，而不是一个，三个，四个或者更多？SMO算法两个变量的选择是怎么来的？SMO算法的基本步骤是什么？以及SVR和SVC都有哪些具体的区别？有没有详细的物理直观的解释和公式比较？</p>
<p>针对以上问题，本文都已经详细作答，希望阅读完本SVM三部曲的朋友能够对SVM有更深的了解和认识。文章中夹杂着很多自己的思考和理解，如有不正确的地方，请多多指正，也希望志同道合之士能够多多交流。</p></div><a class="more" href="/7. 攀登传统机器学习的珠峰-SVM (下)/#more">阅读更多</a><hr></div><div class="recent-post-item"><a class="article-title" href="/6. 攀登传统机器学习的珠峰-SVM (中)/">攀登传统机器学习的珠峰-SVM (中)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-05-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/machine-learning/">machine learning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/机器学习/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/svm/">svm</a></span><div class="content"><p>关于软间隔SVM和非线性SVM，在学习过程中，估计有很多入门的同学会好奇软间隔和硬间隔的区别有没有更合理的解释？软间隔中引入的松弛变量到底是什么？软间隔的优化函数和硬间隔的优化函数化简之后，为什么长得这么类似？有没有更形象的方式来划分软间隔中的支持向量，噪声点和误分类的点？软间隔SVM的代价函数，硬间隔SVM的代价函数和合页损失函数是一致的吗？以及核函数是个什么玩意？核函数的优点到底怎么解释？</p>
<p>下面我将用EM算法的思想去解释软间隔和硬间隔的区别，并用通俗易懂的语言解释松弛变量的几何含义，以及系数C对支持变量的影响。用一张图解释软间隔是怎样区分支持向量，噪声点和误分类的点。对软间隔SVM的代价函数，硬间隔SVM的代价函数和合页损失函数的一致性进行了推导。 之后对特征空间和核函数的核心idea进行了阐述，并分析了核函数的形式来历和那句让人捉摸不透的优点。最后简要介绍了一下几个重要的核函数。</p>
<p>由于文章当中包含很多自己理解的部分，如有不当之处，请多多指正！！！</p></div><a class="more" href="/6. 攀登传统机器学习的珠峰-SVM (中)/#more">阅读更多</a><hr></div><div class="recent-post-item"><a class="article-title" href="/5. 攀登传统机器学习的珠峰-SVM (上)/">攀登传统机器学习的珠峰-SVM (上)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-04-30</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/machine-learning/">machine learning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/机器学习/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/svm/">svm</a></span><div class="content"><p>SVM 可以说是传统机器学习中最难的算法之一，也是很多面试官最喜欢的算法之一。网上有各种各样关于SVM的博客文章，市面上也有很多SVM的书籍。大部分文章，要么仅面向小白，深度不够，要么搞一大堆公式放上去忽悠人。很少能见到对 SVM 算法分析非常透彻的文章。估计很多同学看了很多文章，还是有很多疑问。 SVM的核心idea是什么？它和我们平时见到的逻辑回归有什么区别和联系？几何间隔和函数间隔为什么长成那个样子？SVM定义的间隔和这两种间隔本质上有什么区别和联系？ SVM的那一堆公式都是什么意思？有没有更直观的表述？     下面我会从最常见的感知机和逻辑回归算法入手，分别从机器学习三要素，算法的核心idea来分析 SVM 和逻辑回归以及感知机之间的区别和内在联系，然后讲解 SVM 算法中几个核心的 idea 和推导步骤，SVM中那种间隔定义的根源以及另外一种理解方式， SVM 是怎样“盗取”EM算法的核心思想等等。希望大家读完之后会有一种一览众山小的感觉，同时能体会到SVM算法精妙！    </p>
<p> 很多东西都是个人理解，有不准确的地方，还请大家指正！     </p>
<p>下面让我们来揭开SVM的神秘面纱吧！！！</p></div><a class="more" href="/5. 攀登传统机器学习的珠峰-SVM (上)/#more">阅读更多</a><hr></div><div class="recent-post-item"><a class="article-title" href="/4. 人人都懂EM算法/">人人都懂EM算法</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-04-28</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/machine-learning/">machine learning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/机器学习/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/EM算法/">EM算法</a></span><div class="content"><p>估计有很多入门机器学习的同学在看到EM算法的时候会有种种疑惑：EM算法到底是个什么玩意？它能做什么？它的应用场景是什么？网上的公式推导怎么看不懂？</p>
<p>下面我从极大似然估计开始，过渡到EM算法，讲解EM算法最核心的idea，以及EM算法的具体步骤。鉴于网上很多博客文章都是直接翻译吴恩达的课程笔记内容，有很多推导步骤都是跳跃性的，我会把这些中间步骤弥补上，让大家都能看懂EM算法的推导过程。最后以一个二硬币模型作为EM算法的一个实例收尾。希望阅读本篇文章之后能对EM算法有更深的了解和认识。</p></div><a class="more" href="/4. 人人都懂EM算法/#more">阅读更多</a><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2018 By August</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.3"></script><script src="/js/fancybox.js?version=1.5.3"></script><script src="/js/sidebar.js?version=1.5.3"></script><script src="/js/copy.js?version=1.5.3"></script><script src="/js/fireworks.js?version=1.5.3"></script><script src="/js/transition.js?version=1.5.3"></script><script src="/js/scroll.js?version=1.5.3"></script><script src="/js/head.js?version=1.5.3"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>