<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content=""><meta name="keywords" content=""><meta name="author" content="August,undefined"><meta name="copyright" content="August"><title>Technologies come and technologies go, but insight is forever. | 人工智能随笔</title><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.3"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  localSearch: undefined
} </script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="author-info"><div class="author-info__avatar text-center"><img src="http://p7l96nurm.bkt.clouddn.com/blog/180423/kC2iKEe0K1.jpg"></div><div class="author-info__name text-center">August</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">7</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">5</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">1</span></a></div></div></div><nav class="http://p7l96nurm.bkt.clouddn.com/blog/180426/jaAmCbe3cc.jpg" id="nav" style="background-image: url(http://p7l96nurm.bkt.clouddn.com/blog/180426/jaAmCbe3cc.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">人工智能随笔</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="site-info"><div id="site-title">人工智能随笔</div><div id="site-sub-title">Technologies come and technologies go, but insight is forever.</div></div></nav><div id="content-outer"><div class="layout" id="content-inner"><div class="recent-post-item"><a class="article-title" href="/7. 攀登传统机器学习的珠峰-SVM (下)/">攀登传统机器学习的珠峰-SVM (下)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-05-04</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/machine-learning/">machine learning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/机器学习/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/svm/">svm</a></span><div class="content"><p>本节是SVM系列三部曲的最后一部分。主要讲了SMO算法，SVR算法，sklearn中SVM算法的参数介绍和调参建议。网上有很多SMO算法的文章，有的要么讲解的比较浅显，要么就是一堆公式的堆砌，让人看完之后会有各种疑问，比如：SMO算法和EM算法有什么异同点？坐标上升(下降)法与梯度下降(上升)法有什么异同点？什么时候用坐标上升(降法)？什么时候用梯度下降法(上升)？SMO为什么采用两个变量迭代，而不是一个，三个，四个或者更多？SMO算法两个变量的选择是怎么来的？SMO算法的基本步骤是什么？以及SVR和SVC都有哪些具体的区别？有没有详细的物理直观的解释和公式比较？</p>
<p>针对以上问题，本文都已经详细作答，希望阅读完本SVM三部曲的朋友能够对SVM有更深的了解和认识。文章中夹杂着很多自己的思考和理解，如有不正确的地方，请多多指正，也希望志同道合之士能够多多交流。</p></div><a class="more" href="/7. 攀登传统机器学习的珠峰-SVM (下)/#more">阅读更多</a><hr></div><div class="recent-post-item"><a class="article-title" href="/6. 攀登传统机器学习的珠峰-SVM (中)/">攀登传统机器学习的珠峰-SVM (中)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-05-02</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/machine-learning/">machine learning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/机器学习/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/svm/">svm</a></span><div class="content"><p>关于软间隔SVM和非线性SVM，在学习过程中，估计有很多入门的同学会好奇软间隔和硬间隔的区别有没有更合理的解释？软间隔中引入的松弛变量到底是什么？软间隔的优化函数和硬间隔的优化函数化简之后，为什么长得这么类似？有没有更形象的方式来划分软间隔中的支持向量，噪声点和误分类的点？软间隔SVM的代价函数，硬间隔SVM的代价函数和合页损失函数是一致的吗？以及核函数是个什么玩意？核函数的优点到底怎么解释？</p>
<p>下面我将用EM算法的思想去解释软间隔和硬间隔的区别，并用通俗易懂的语言解释松弛变量的几何含义，以及系数C对支持变量的影响。用一张图解释软间隔是怎样区分支持向量，噪声点和误分类的点。对软间隔SVM的代价函数，硬间隔SVM的代价函数和合页损失函数的一致性进行了推导。 之后对特征空间和核函数的核心idea进行了阐述，并分析了核函数的形式来历和那句让人捉摸不透的优点。最后简要介绍了一下几个重要的核函数。</p>
<p>由于文章当中包含很多自己理解的部分，如有不当之处，请多多指正！！！</p></div><a class="more" href="/6. 攀登传统机器学习的珠峰-SVM (中)/#more">阅读更多</a><hr></div><div class="recent-post-item"><a class="article-title" href="/5. 攀登传统机器学习的珠峰-SVM (上)/">攀登传统机器学习的珠峰-SVM (上)</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-04-30</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/machine-learning/">machine learning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/机器学习/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/svm/">svm</a></span><div class="content"><p>SVM 可以说是传统机器学习中最难的算法之一，也是很多面试官最喜欢的算法之一。网上有各种各样关于SVM的博客文章，市面上也有很多SVM的书籍。大部分文章，要么仅面向小白，深度不够，要么搞一大堆公式放上去忽悠人。很少能见到对 SVM 算法分析非常透彻的文章。估计很多同学看了很多文章，还是有很多疑问。 SVM的核心idea是什么？它和我们平时见到的逻辑回归有什么区别和联系？几何间隔和函数间隔为什么长成那个样子？SVM定义的间隔和这两种间隔本质上有什么区别和联系？ SVM的那一堆公式都是什么意思？有没有更直观的表述？     下面我会从最常见的感知机和逻辑回归算法入手，分别从机器学习三要素，算法的核心idea来分析 SVM 和逻辑回归以及感知机之间的区别和内在联系，然后讲解 SVM 算法中几个核心的 idea 和推导步骤，SVM中那种间隔定义的根源以及另外一种理解方式， SVM 是怎样“盗取”EM算法的核心思想等等。希望大家读完之后会有一种一览众山小的感觉，同时能体会到SVM算法精妙！    </p>
<p> 很多东西都是个人理解，有不准确的地方，还请大家指正！     </p>
<p>下面让我们来揭开SVM的神秘面纱吧！！！</p></div><a class="more" href="/5. 攀登传统机器学习的珠峰-SVM (上)/#more">阅读更多</a><hr></div><div class="recent-post-item"><a class="article-title" href="/4. 人人都懂EM算法/">人人都懂EM算法</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-04-28</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/machine-learning/">machine learning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/机器学习/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/EM算法/">EM算法</a></span><div class="content"><p>估计有很多入门机器学习的同学在看到EM算法的时候会有种种疑惑：EM算法到底是个什么玩意？它能做什么？它的应用场景是什么？网上的公式推导怎么看不懂？</p>
<p>下面我从极大似然估计开始，过渡到EM算法，讲解EM算法最核心的idea，以及EM算法的具体步骤。鉴于网上很多博客文章都是直接翻译吴恩达的课程笔记内容，有很多推导步骤都是跳跃性的，我会把这些中间步骤弥补上，让大家都能看懂EM算法的推导过程。最后以一个二硬币模型作为EM算法的一个实例收尾。希望阅读本篇文章之后能对EM算法有更深的了解和认识。</p></div><a class="more" href="/4. 人人都懂EM算法/#more">阅读更多</a><hr></div><div class="recent-post-item"><a class="article-title" href="/3. 决策树的进化史/">决策树的进化史</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-04-26</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/machine-learning/">machine learning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/机器学习/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/评估指标/">评估指标</a></span><div class="content"><p>决策树是我们最常见的机器学习算法，和网上的博客和教材都介绍了决策树的不同，本文以“分而治之”的思想介绍了决策树的来源，包括ID3，C4.5 和 CART树，分别讲解了这些算法的原理，几何意义和物理意义，让大家更加清楚三种决策树短发的本质和由来。里面有一些自己的观点和体会，如有不当，请大家指正。</p></div><a class="more" href="/3. 决策树的进化史/#more">阅读更多</a><hr></div><div class="recent-post-item"><a class="article-title" href="/2. lr正则化的直观理解/">直观理解正则化</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-04-24</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/machine-learning/">machine learning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/机器学习/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/正则化/">正则化</a></span><div class="content"><p>本文从无约束的优化问题、有等式约束的优化问题和等式和不等式约束的优化问题三种优化问题的几何意义入手，讲解了L1 正则化和 L2 正则化的几何含义，进而推广至正则化的一般形式： $L_q$ 正则化 和  Elastic Net 正则化，得到L1 正则化和 L2 正则化的折中版本。</p></div><a class="more" href="/2. lr正则化的直观理解/#more">阅读更多</a><hr></div><div class="recent-post-item"><a class="article-title" href="/1. 机器学习评估指标的前世今生/">机器学习评估指标的前世今生</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-04-22</time><span class="article-meta"><span class="article-meta__separator">|</span><i class="fa fa-inbox article-meta__icon" aria-hidden="true"></i><a class="article-meta__categories" href="/categories/machine-learning/">machine learning</a></span><span class="article-meta tags"><span class="article-meta__separator">|</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/机器学习/">机器学习</a><span class="article-meta__link">-</span><i class="fa fa-tag article-meta__icon" aria-hidden="true"></i><a class="article-meta__tags" href="/tags/评估指标/">评估指标</a></span><div class="content"><p>很多同学在入门机器学习的时候都会好奇，这么多的机器学习指标，到底要用哪个？这些指标的含义和优缺点是什么？他们之间有没有联系？像AUC这种常用的指标到底是什么意思，它的核心idea又是什么？它是怎样计算出来的？</p>
<p>下面，我会用通俗易懂的语言，介绍不同的机器学习评估指标的具体含义，优缺点，以及它们之间的联系，将常用的机器学习评估指标串成一条线。然后重点介绍了AUC的来历，核心的idea以及计算过程。并简要介绍了评估指标和代价函数之间的关系和区别。里面有一些自己的观点和体会，如有不当，请大家指正。废话少说，上正文哈！！！</p></div><a class="more" href="/1. 机器学习评估指标的前世今生/#more">阅读更多</a><hr></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2013 - 2018 By August</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_site_uv"><i class="fa fa-user"></i><span id="busuanzi_value_site_uv"></span><span></span></span><span class="footer-separator">|</span><span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_site_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.5.3"></script><script src="/js/fancybox.js?version=1.5.3"></script><script src="/js/sidebar.js?version=1.5.3"></script><script src="/js/copy.js?version=1.5.3"></script><script src="/js/fireworks.js?version=1.5.3"></script><script src="/js/transition.js?version=1.5.3"></script><script src="/js/scroll.js?version=1.5.3"></script><script src="/js/head.js?version=1.5.3"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>